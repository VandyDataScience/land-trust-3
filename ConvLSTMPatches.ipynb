{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e39dec4532b5378",
   "metadata": {},
   "source": [
    "# ConvLSTM for Patches\n",
    "Trains a convolutional LSTM. The model input is a stack of 64x64 patches of the total area of interest, and the model output is a 64x64 patch with the predicted land type at the next time step. Requires about 12 GB RAM to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d736d03a49c2c0c4",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-05T20:03:02.122457Z",
     "start_time": "2024-03-05T20:02:50.753830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras_core as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e7627eaae1552e",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-03-05T20:03:02.127995Z",
     "start_time": "2024-03-05T20:03:02.121651Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fee8d50032de8",
   "metadata": {},
   "source": [
    "Define parameters for patch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90455e559c6e929f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:09:19.023231Z",
     "start_time": "2024-03-04T23:09:19.017135Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATCH_SIZE = 64  # Size of each patch in pixels\n",
    "OVERLAP_SIZE = 32  # Number of pixels to advance before accessing the next patch\n",
    "MAX_EMPTY_RATIO = 0.4  # Maximum percent of pixels in the image that can be zero\n",
    "MIN_CHANGE_RATIO = 0.05  # Minimum percent of pixels that must change from the earliest to latest timestamp in the dataset\n",
    "TIME_STEPS = 5  # Number of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d227131e33c9cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:09:31.815944Z",
     "start_time": "2024-03-04T23:09:19.020975Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 17975, 35165)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for f in glob('data/CONUS20*_ClipAOI*.tif'):\n",
    "    with rasterio.open(f) as ds:\n",
    "        data = ds.read(1)\n",
    "        images.append(data)\n",
    "images = np.array(images)\n",
    "n_times = images.shape[0]\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1dcc13feb31fe",
   "metadata": {},
   "source": [
    "Compute 2D prefix sum arrays for the entire large image. When passing patches to the model during training, we want to exclude patches where the entire image or the majority of pixels are out of bounds (zero), and also images where the terrain is almost completely unchanged. Calculating the prefix sum arrays for the entire large image will allow fast querying of the number of zero pixels in any given patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bb1aa44efc9473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:09:41.509150Z",
     "start_time": "2024-03-04T23:09:31.814058Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17975, 35165)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = images[0, :, :]\n",
    "zero_prefix = np.zeros_like(image, dtype=np.uint32)\n",
    "zero_prefix[image == 0] = 1\n",
    "zero_prefix = np.cumsum(np.cumsum(zero_prefix, axis=0, dtype=np.uint32), axis=1, dtype=np.uint32)\n",
    "zero_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acd74816fc1daab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:09:41.517361Z",
     "start_time": "2024-03-04T23:09:41.499952Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_zero_pixels(i, j):\n",
    "    \"\"\"Calculates the number of zero pixels in the patch with corner at (i, j).\"\"\"\n",
    "    zeros = zero_prefix[i + PATCH_SIZE - 1, j + PATCH_SIZE - 1]\n",
    "    if i > 0 and j > 0:\n",
    "        zeros += zero_prefix[i - 1, j - 1]\n",
    "    if i > 0:\n",
    "        zeros -= zero_prefix[i - 1, j + PATCH_SIZE - 1]\n",
    "    if j > 0:\n",
    "        zeros -= zero_prefix[i + PATCH_SIZE - 1, j - 1]\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49198f93989b7f37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "change_prefix = np.zeros_like(image, dtype=np.uint32)\n",
    "change_prefix[images[0, :, :] != images[-1, :, :]] = 1\n",
    "change_prefix = np.cumsum(np.cumsum(change_prefix, axis=0, dtype=np.uint32), axis=1, dtype=np.uint32)\n",
    "change_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df104417d7d8fd38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_changed_pixels(i, j):\n",
    "    \"\"\"Calculates the number of changed pixels in the patch with corner at (i, j).\"\"\"\n",
    "    changes = change_prefix[i + PATCH_SIZE - 1, j + PATCH_SIZE - 1]\n",
    "    if i > 0 and j > 0:\n",
    "        changes += change_prefix[i - 1, j - 1]\n",
    "    if i > 0:\n",
    "        changes -= change_prefix[i - 1, j + PATCH_SIZE - 1]\n",
    "    if j > 0:\n",
    "        changes -= change_prefix[i + PATCH_SIZE - 1, j - 1]\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b0019570d12c9",
   "metadata": {},
   "source": [
    "Determine the possible categories and normalize them to integer values starting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7814a2bf49903c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories, counts = np.unique(image, return_counts=True)\n",
    "np.save('categories.npy', categories)\n",
    "n_categories = categories.shape[0]\n",
    "category_map = {categories[i]: i for i in range(n_categories)}\n",
    "percents = counts / image.size * 100\n",
    "del image\n",
    "plt.bar(list(map(str, categories)), percents)\n",
    "plt.title('Land Type Distribution')\n",
    "plt.ylabel('Percent')\n",
    "plt.show()\n",
    "category_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6517dbb7bf2b9fc",
   "metadata": {},
   "source": [
    "Using the prefix sum arrays, find the indices of every patch in the dataset that lies in the area of interest and has enough changed pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addc3d11c0c752",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(0, images.shape[1] - OVERLAP_SIZE, OVERLAP_SIZE):\n",
    "    for j in range(0, images.shape[2] - OVERLAP_SIZE, OVERLAP_SIZE):\n",
    "        zeros = get_zero_pixels(i, j)\n",
    "        if zeros >= PATCH_SIZE * PATCH_SIZE * MAX_EMPTY_RATIO:\n",
    "            continue\n",
    "        changes = get_changed_pixels(i, j)\n",
    "        if changes >= PATCH_SIZE * PATCH_SIZE * MIN_CHANGE_RATIO:\n",
    "            continue\n",
    "        indices.append((i, j))\n",
    "del zero_prefix, change_prefix\n",
    "indices = np.array(indices)\n",
    "rng.shuffle(indices)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd75bd84fc74dda",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Define training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1a3197db4502b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:09:41.518913Z",
     "start_time": "2024-03-04T23:09:41.504836Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6adc593066543b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_size = int(indices.shape[0] * VAL_SPLIT)\n",
    "val_indices = indices[:val_size, :]\n",
    "train_indices = indices[val_size:, :]\n",
    "train_indices.shape, val_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60352c5d30c667af",
   "metadata": {},
   "source": [
    "Build the dataset using the list of patch indices. The full dataset is generated in-place using a generator function because it would be too large to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f5a33efca1c30f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:11:06.823797Z",
     "start_time": "2024-03-04T23:11:06.808999Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    encoded = np.zeros(x.shape + (n_categories,), dtype=np.uint8)\n",
    "    for category, index in category_map.items():\n",
    "        category_mask = (x == category)\n",
    "        encoded[category_mask, index] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_data(indices):\n",
    "    sparse_encoder = np.vectorize(lambda x: category_map[x], otypes=[np.uint8])\n",
    "    for i, j in indices:\n",
    "        for k in range(n_times - TIME_STEPS):\n",
    "            x = one_hot(images[k:k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            y = sparse_encoder(images[k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60c6c86b244a1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: get_data(train_indices),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE), dtype=tf.uint8)\n",
    "    )\n",
    ")\n",
    "# Tell Keras the full size of the dataset so we get ETA in the progress bar\n",
    "train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(train_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: get_data(val_indices),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE), dtype=tf.uint8)\n",
    "    )\n",
    ")\n",
    "val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(val_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "val_ds = val_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e754bd7c1d08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories)),\n",
    "    keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(n_categories, kernel_size=(3, 3), padding='same', activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name='acc')\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e6056dff125d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787c74bb561f7bc",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('lttn-convlstm-patches.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791915d7fc0cdd28",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "Load the saved model (optional if run from the same session used to train above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79df7ffe019d6493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:10:07.287881Z",
     "start_time": "2024-03-04T23:10:06.891175Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:42:22.297305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20758 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ conv_lstm2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">184,576</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv_lstm2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,232</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ conv_lstm2d (\u001B[38;5;33mConvLSTM2D\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │    \u001B[38;5;34m184,576\u001B[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ batch_normalization             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv_lstm2d_1 (\u001B[38;5;33mConvLSTM2D\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m)        │    \u001B[38;5;34m295,168\u001B[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m16\u001B[0m)        │      \u001B[38;5;34m9,232\u001B[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,467,442</span> (5.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,467,442\u001B[0m (5.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">489,104</span> (1.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m489,104\u001B[0m (1.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m128\u001B[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">978,210</span> (3.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m978,210\u001B[0m (3.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.saving.load_model('lttn-convlstm-patches.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102001a369b559ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:10:08.891763Z",
     "start_time": "2024-03-04T23:10:08.883662Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 11: 1,\n",
       " 21: 2,\n",
       " 22: 3,\n",
       " 23: 4,\n",
       " 24: 5,\n",
       " 31: 6,\n",
       " 41: 7,\n",
       " 42: 8,\n",
       " 43: 9,\n",
       " 52: 10,\n",
       " 71: 11,\n",
       " 81: 12,\n",
       " 82: 13,\n",
       " 90: 14,\n",
       " 95: 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = np.load('categories.npy')\n",
    "n_categories = categories.shape[0]\n",
    "category_map = {categories[i]: i for i in range(n_categories)}\n",
    "category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fb1044-519e-4fa2-b740-4f714917b09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFER_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928518c35eff450",
   "metadata": {},
   "source": [
    "Run the model to predict the landscape of a random patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ac61b70a73ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T00:46:43.897652Z",
     "start_time": "2024-02-27T00:46:43.709937Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 10192 # random.randint(2000, 16000)\n",
    "y = 14735 # random.randint(3000, 32000)\n",
    "patch = images[-TIME_STEPS:, x:x + PATCH_SIZE, y:y + PATCH_SIZE]\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 50))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(patch[i, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c409d86feab4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T00:47:00.283553Z",
     "start_time": "2024-02-27T00:46:59.375927Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch = one_hot(patch)\n",
    "patch = np.expand_dims(patch, axis=0)\n",
    "result = model.predict(patch)\n",
    "result = np.squeeze(result, axis=0)\n",
    "result = np.argmax(result, axis=-1)\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827515618af7e20",
   "metadata": {},
   "source": [
    "Set parameters for generating the large prediction map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8565a88c8becd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:01:32.203630Z",
     "start_time": "2024-03-04T23:01:32.190264Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 10192 # random.randint(2000, 16000)\n",
    "y = 14735 # random.randint(3000, 32000)\n",
    "size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea515835d46cf89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:10:15.189647Z",
     "start_time": "2024-03-04T23:10:15.169934Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_generator(step):\n",
    "    for i in range(0, PATCH_SIZE * size - (PATCH_SIZE - step), step):\n",
    "        for j in range(0, PATCH_SIZE * size - (PATCH_SIZE - step), step):\n",
    "            patch = images[-TIME_STEPS:, x + i:x + i + PATCH_SIZE, y + j:y + j + PATCH_SIZE]\n",
    "            patch = one_hot(patch)\n",
    "            yield i, j, patch\n",
    "\n",
    "def batch_patch_generator(gen):\n",
    "    coords = []\n",
    "    batch = []\n",
    "    n = 0\n",
    "    for i, j, patch in gen:\n",
    "        coords.append((i, j))\n",
    "        batch.append(patch)\n",
    "        n += 1\n",
    "        if n == INFER_BATCH_SIZE:\n",
    "            yield np.array(coords), np.stack(batch)\n",
    "            coords = []\n",
    "            batch = []\n",
    "            n = 0\n",
    "    if n:\n",
    "        yield np.array(coords), np.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bed4045a4d2d62",
   "metadata": {},
   "source": [
    "Construct a large map of predictions by running the model on several consecutive patches without overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0075934694d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T00:47:45.038285Z",
     "start_time": "2024-02-27T00:47:40.122377Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = np.zeros((PATCH_SIZE * size, PATCH_SIZE * size), dtype=np.uint8)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(range(0, PATCH_SIZE * size, PATCH_SIZE)) ** 2 / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(patch_generator(PATCH_SIZE))\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    result = np.argmax(result, axis=-1)\n",
    "    for i in range(len(coords)):\n",
    "        image[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] = result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f75633600cb5c",
   "metadata": {},
   "source": [
    "Construct a large map of predictions with overlap. The probability distributions at each pixel are summed with contribution from all overlapping patches before `argmax` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05bc37c28256c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = np.zeros((PATCH_SIZE * size, PATCH_SIZE * size, n_categories), dtype=np.float32)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(range(0, PATCH_SIZE * size - OVERLAP_SIZE, OVERLAP_SIZE)) ** 2 / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(patch_generator(OVERLAP_SIZE))\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    for i in range(len(coords)):\n",
    "        probs[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] += result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "image = np.argmax(probs, axis=-1)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7b5c64da46e87",
   "metadata": {},
   "source": [
    "Build a map over the entire area of interest. Be sure to run the cell creating `zero_prefix` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170a9242d3f95636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:10:23.123004Z",
     "start_time": "2024-03-04T23:10:22.792211Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404798, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = []\n",
    "for i in range(0, images.shape[1] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "    for j in range(0, images.shape[2] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "        zeros = get_zero_pixels(i, j)\n",
    "        if zeros == PATCH_SIZE * PATCH_SIZE:\n",
    "            continue  # Don't need to run any prediction if the slice is entirely blank\n",
    "        indices.append((i, j))\n",
    "del zero_prefix\n",
    "indices = np.array(indices)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "929acea6a9e89e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T23:10:54.359910Z",
     "start_time": "2024-03-04T23:10:54.321845Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_patch_generator():\n",
    "    for i, j in indices:\n",
    "        patch = images[-TIME_STEPS:, i:i + PATCH_SIZE, j:j + PATCH_SIZE]\n",
    "        patch = one_hot(patch)\n",
    "        yield i, j, patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5e169bae2ccc08",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:44:30.223903: I external/local_xla/xla/service/service.cc:168] XLA service 0x5635bb49b8b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-05 17:44:30.223944: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2024-03-05 17:44:30.279582: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-05 17:44:30.500511: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709660672.780694    3313 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m3163/3163\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1943s\u001B[0m 613ms/batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17975, 35165)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.zeros(images.shape[1:] + (n_categories,), dtype=np.float32)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(indices) / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(full_patch_generator())\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    for i in range(len(coords)):\n",
    "        probs[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] += result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "image = np.argmax(probs, axis=-1)\n",
    "del probs\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adf3024e-8ffe-444a-b60e-3e29c2cb238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17975, 35165)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.vectorize(lambda x: categories[x])(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c2a8baef395ffd",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('full-map.npy', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462b4117-03bd-43c3-aa94-11154f0d6d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, 11, 21, 22, 23, 24, 31, 41, 42, 43, 52, 71, 81, 82, 90, 95],\n",
       "       dtype=uint8),\n",
       " array([220751342,   8437590,  19866658,  10147817,   4150609,   1050711,\n",
       "           678983, 167194841,  14188330,  30559197,    116270,    554884,\n",
       "         75472235,  63512129,  14959503,    449776]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(image, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the numpy array and save the data as a GeoTIFF file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfcc274e582e0181"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(17975, 35165)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.load('full-map.npy')\n",
    "image.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:35:42.032099Z",
     "start_time": "2024-03-05T20:35:41.728375Z"
    }
   },
   "id": "143be246c12991f5",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb80980b-c62b-46e6-9e6c-64d14a4a4fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T20:37:44.566462Z",
     "start_time": "2024-03-05T20:37:43.605020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'uint8', 'nodata': None, 'width': 35165, 'height': 17975, 'count': 1, 'crs': CRS.from_epsg(5070), 'transform': Affine(29.999746813208393, 0.0, 364730.7691300305,\n",
      "       0.0, -29.99934164631371, 1737544.4746212494), 'blockxsize': 128, 'blockysize': 128, 'tiled': True, 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open('data/CONUS2001_ClipAOI.tif') as src:\n",
    "    profile = src.profile.copy()\n",
    "    with rasterio.open('prediction-1.tif', 'w', **profile) as dst:\n",
    "        dst.write(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa97d813e3bf357"
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m117"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
