{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ConvLSTM for Patches\n",
    "Trains a convolutional LSTM. The model input is a stack of 64x64 patches of the total area of interest, and the model output is a 64x64 patch with the predicted land type at the next time step. Requires about 12 GB RAM to run."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e39dec4532b5378"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras_core as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d736d03a49c2c0c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e7627eaae1552e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define parameters for patch size."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256fee8d50032de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PATCH_SIZE = 64  # Size of each patch in pixels\n",
    "OVERLAP_SIZE = 64  # Number of pixels to advance before accessing the next patch\n",
    "MAX_EMPTY_RATIO = 0.4  # Maximum percent of pixels in the image that can be zero\n",
    "TIME_STEPS = 5  # Number of time steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90455e559c6e929f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Generation\n",
    "Read all input files and stack them on top of each other to create a large numpy array."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3acba28cf6df781"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images = []\n",
    "for f in glob('data/CONUS20*_ClipAOI*.tif'):\n",
    "    with rasterio.open(f) as ds:\n",
    "        data = ds.read(1)\n",
    "        images.append(data)\n",
    "images = np.array(images)\n",
    "n_times = images.shape[0]\n",
    "images.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d227131e33c9cc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute a 2D prefix sum array for the entire large image. When passing patches to the model during training, we want to exclude patches where the entire image or the majority of pixels are out of bounds (zero). Calculating the prefix sum array for the entire large image will allow fast querying of the number of zero pixels in any given patch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b1dcc13feb31fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = images[0, :, :]\n",
    "prefix = np.zeros_like(image, dtype=np.uint32)\n",
    "prefix[image == 0] = 1\n",
    "prefix = np.cumsum(np.cumsum(prefix, axis=0, dtype=np.uint32), axis=1, dtype=np.uint32)\n",
    "prefix.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32bb1aa44efc9473"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_zero_pixels(i, j):\n",
    "    \"\"\"Calculates the number of zero pixels in the patch with corner at (i, j).\"\"\"\n",
    "    zeros = prefix[i + PATCH_SIZE - 1, j + PATCH_SIZE - 1]\n",
    "    if i > 0 and j > 0:\n",
    "        zeros += prefix[i - 1, j - 1]\n",
    "    if i > 0:\n",
    "        zeros -= prefix[i - 1, j + PATCH_SIZE - 1]\n",
    "    if j > 0:\n",
    "        zeros -= prefix[i + PATCH_SIZE - 1, j - 1]\n",
    "    return zeros"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1acd74816fc1daab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determine the possible categories and normalize them to integer values starting at 0."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e34b0019570d12c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categories, counts = np.unique(image, return_counts=True)\n",
    "n_categories = categories.shape[0]\n",
    "category_map = {categories[i]: i for i in range(n_categories)}\n",
    "percents = counts / image.size * 100\n",
    "del image\n",
    "plt.bar(list(map(str, categories)), percents)\n",
    "plt.title('Land Type Distribution')\n",
    "plt.ylabel('Percent')\n",
    "plt.show()\n",
    "category_map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7814a2bf49903c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the prefix sums array, find the indices of every patch in the dataset that lies in the area of interest."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6517dbb7bf2b9fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(0, images.shape[1] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "    for j in range(0, images.shape[2] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "        zeros = get_zero_pixels(i, j)\n",
    "        if zeros >= PATCH_SIZE * PATCH_SIZE * MAX_EMPTY_RATIO:\n",
    "            continue\n",
    "        indices.append((i, j))\n",
    "del prefix  # Not needed anymore\n",
    "indices = np.array(indices)\n",
    "rng.shuffle(indices)\n",
    "indices.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2addc3d11c0c752"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model\n",
    "Define training parameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd75bd84fc74dda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "VAL_SPLIT = 0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffd1a3197db4502b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_size = int(indices.shape[0] * VAL_SPLIT)\n",
    "val_indices = indices[:val_size, :]\n",
    "train_indices = indices[val_size:, :]\n",
    "train_indices.shape, val_indices.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f6adc593066543b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the dataset using the list of patch indices. The full dataset is generated in-place using a generator function because it would be too large to fit in memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60352c5d30c667af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    encoded = np.zeros(x.shape + (n_categories,), dtype=np.uint8)\n",
    "    for category, index in category_map.items():\n",
    "        category_mask = (x == category)\n",
    "        encoded[category_mask, index] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_data(indices):\n",
    "    sparse_encoder = np.vectorize(lambda x: category_map[x], otypes=[np.uint8])\n",
    "    for i, j in indices:\n",
    "        for k in range(n_times - TIME_STEPS):\n",
    "            x = one_hot(images[k:k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            y = sparse_encoder(images[k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            yield x, y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1f5a33efca1c30f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: get_data(train_indices),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE), dtype=tf.uint8)\n",
    "    )\n",
    ")\n",
    "train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(train_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: get_data(val_indices),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE), dtype=tf.uint8)\n",
    "    )\n",
    ")\n",
    "val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(val_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "val_ds = val_ds.batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f60c6c86b244a1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories)),\n",
    "    keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(n_categories, kernel_size=(3, 3), padding='same', activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name='acc')\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "611e754bd7c1d08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f6e6056dff125d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7787c74bb561f7bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
