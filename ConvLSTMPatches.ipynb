{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e39dec4532b5378",
   "metadata": {},
   "source": [
    "# ConvLSTM for Patches\n",
    "Trains a convolutional LSTM. The model input is a stack of 64x64 patches of the total area of interest, and the model output is a 64x64 patch with the predicted land type at the next time step. Requires about 12 GB RAM to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736d03a49c2c0c4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras_core as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7627eaae1552e",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fee8d50032de8",
   "metadata": {},
   "source": [
    "Define parameters for patch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90455e559c6e929f",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATCH_SIZE = 64  # Size of each patch in pixels\n",
    "OVERLAP_SIZE = 32  # Number of pixels to advance before accessing the next patch\n",
    "MAX_EMPTY_RATIO = 0.4  # Maximum percent of pixels in the image that can be zero\n",
    "MIN_CHANGE_RATIO = 0.2  # Minimum percent of pixels that must change from the earliest to latest timestamp in the dataset\n",
    "TIME_STEPS = 5  # Number of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227131e33c9cc1",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "files = sorted(glob('data/CONUS20*_ClipAOI*.tif'))\n",
    "for f in files:\n",
    "    with rasterio.open(f) as ds:\n",
    "        data = ds.read(1)\n",
    "        images.append(data)\n",
    "images = np.array(images)\n",
    "n_times = images.shape[0]\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9baf5e5365a99",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with rasterio.open('data/slope.tif') as ds:\n",
    "    slopes = ds.read(1)\n",
    "slopes[slopes <= 0] = 0\n",
    "slopes = np.expand_dims(slopes, axis=-1)\n",
    "slopes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1dcc13feb31fe",
   "metadata": {},
   "source": [
    "Compute 2D prefix sum arrays for the entire large image. When passing patches to the model during training, we want to exclude patches where the entire image or the majority of pixels are out of bounds (zero), and also images where the terrain is almost completely unchanged. Calculating the prefix sum arrays for the entire large image will allow fast querying of the number of zero pixels in any given patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb1aa44efc9473",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = images[0, :, :]\n",
    "zero_prefix = np.zeros_like(image, dtype=np.uint32)\n",
    "zero_prefix[image == 0] = 1\n",
    "zero_prefix = np.cumsum(np.cumsum(zero_prefix, axis=0, dtype=np.uint32), axis=1, dtype=np.uint32)\n",
    "zero_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd74816fc1daab",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_zero_pixels(i, j):\n",
    "    \"\"\"Calculates the number of zero pixels in the patch with corner at (i, j).\"\"\"\n",
    "    zeros = zero_prefix[i + PATCH_SIZE - 1, j + PATCH_SIZE - 1]\n",
    "    if i > 0 and j > 0:\n",
    "        zeros += zero_prefix[i - 1, j - 1]\n",
    "    if i > 0:\n",
    "        zeros -= zero_prefix[i - 1, j + PATCH_SIZE - 1]\n",
    "    if j > 0:\n",
    "        zeros -= zero_prefix[i + PATCH_SIZE - 1, j - 1]\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49198f93989b7f37",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "change_prefix = np.zeros_like(image, dtype=np.uint32)\n",
    "change_prefix[images[0, :, :] != images[-1, :, :]] = 1\n",
    "change_prefix = np.cumsum(np.cumsum(change_prefix, axis=0, dtype=np.uint32), axis=1, dtype=np.uint32)\n",
    "change_prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df104417d7d8fd38",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_changed_pixels(i, j):\n",
    "    \"\"\"Calculates the number of changed pixels in the patch with corner at (i, j).\"\"\"\n",
    "    changes = change_prefix[i + PATCH_SIZE - 1, j + PATCH_SIZE - 1]\n",
    "    if i > 0 and j > 0:\n",
    "        changes += change_prefix[i - 1, j - 1]\n",
    "    if i > 0:\n",
    "        changes -= change_prefix[i - 1, j + PATCH_SIZE - 1]\n",
    "    if j > 0:\n",
    "        changes -= change_prefix[i + PATCH_SIZE - 1, j - 1]\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b0019570d12c9",
   "metadata": {},
   "source": [
    "Determine the possible categories and normalize them to integer values starting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7814a2bf49903c3",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories, counts = np.unique(image, return_counts=True)\n",
    "np.save('categories.npy', categories)\n",
    "n_categories = categories.shape[0]\n",
    "category_map = {categories[i]: i for i in range(n_categories)}\n",
    "percents = counts / image.size * 100\n",
    "del image\n",
    "plt.bar(list(map(str, categories)), percents)\n",
    "plt.title('Land Type Distribution')\n",
    "plt.ylabel('Percent')\n",
    "plt.show()\n",
    "category_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6517dbb7bf2b9fc",
   "metadata": {},
   "source": [
    "Using the prefix sum arrays, find the indices of every patch in the dataset that lies in the area of interest and has enough changed pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addc3d11c0c752",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(0, images.shape[1] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "    for j in range(0, images.shape[2] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "        zeros = get_zero_pixels(i, j)\n",
    "        if zeros >= PATCH_SIZE * PATCH_SIZE * MAX_EMPTY_RATIO:\n",
    "            continue\n",
    "        changes = get_changed_pixels(i, j)\n",
    "        if changes < PATCH_SIZE * PATCH_SIZE * MIN_CHANGE_RATIO:\n",
    "            continue\n",
    "        indices.append((i, j))\n",
    "del zero_prefix, change_prefix\n",
    "indices = np.array(indices)\n",
    "rng.shuffle(indices)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd75bd84fc74dda",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Define training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1a3197db4502b",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "VAL_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6adc593066543b",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_size = int(indices.shape[0] * VAL_SPLIT)\n",
    "val_indices = indices[:val_size, :]\n",
    "train_indices = indices[val_size:, :]\n",
    "train_indices.shape, val_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60352c5d30c667af",
   "metadata": {},
   "source": [
    "Build the dataset using the list of patch indices. The full dataset is generated in-place using a generator function because it would be too large to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5a33efca1c30f",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    encoded = np.zeros(x.shape + (n_categories,), dtype=np.uint8)\n",
    "    for category, index in category_map.items():\n",
    "        category_mask = (x == category)\n",
    "        encoded[category_mask, index] = 1\n",
    "    return encoded\n",
    "\n",
    "def get_data(indices):\n",
    "    sparse_encoder = np.vectorize(lambda x: category_map[x], otypes=[np.uint8])\n",
    "    for i, j in indices:\n",
    "        for k in range(n_times - TIME_STEPS):\n",
    "            land = one_hot(images[k:k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            slope = slopes[i:i + PATCH_SIZE, j:j + PATCH_SIZE, :]\n",
    "            x = {'land': land, 'slope': slope}\n",
    "            y = sparse_encoder(images[k + TIME_STEPS, i:i + PATCH_SIZE, j:j + PATCH_SIZE])\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "np.save('val-indices.npy', val_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed10ae95c3b4d79"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds_sig = (\n",
    "    {\n",
    "        'land': tf.TensorSpec(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), dtype=tf.float32),\n",
    "        'slope': tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE, 1), dtype=tf.float32)\n",
    "    },\n",
    "    tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE), dtype=tf.uint8)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13f9b214849b49b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60c6c86b244a1d",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(lambda: get_data(train_indices), output_signature=ds_sig)\n",
    "# Tell Keras the full size of the dataset so we get ETA in the progress bar\n",
    "train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(train_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: get_data(val_indices), output_signature=ds_sig)\n",
    "val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(val_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "val_ds = val_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e754bd7c1d08",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "land_input = keras.layers.Input(shape=(TIME_STEPS, PATCH_SIZE, PATCH_SIZE, n_categories), name='land')\n",
    "slope_input = keras.layers.Input(shape=(PATCH_SIZE, PATCH_SIZE, 1), name='slope')\n",
    "x1 = keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(land_input)\n",
    "x1, = keras.layers.BatchNormalization()(x1),\n",
    "x1 = keras.layers.ConvLSTM2D(64, kernel_size=(3, 3), padding='same', activation='relu')(x1)\n",
    "x2 = keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu')(slope_input)\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "output = keras.layers.Conv2D(n_categories, kernel_size=(3, 3), padding='same', activation='softmax')(x)\n",
    "model = keras.Model(inputs=(land_input, slope_input), outputs=output)\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name='acc')\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7998bd1bfb92f1",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e6056dff125d3",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787c74bb561f7bc",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('lttn-convlstm-patches-slope.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d49f60a98a3e",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Load the saved model (optional if run from the same session used to train above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a87a0476371fe0",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.saving.load_model('lttn-convlstm-patches-slope.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6ba57357759e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = np.load('categories.npy')\n",
    "n_categories = categories.shape[0]\n",
    "category_map = {categories[i]: i for i in range(n_categories)}\n",
    "category_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e99a7a6e7f060",
   "metadata": {},
   "source": [
    "Run the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df591600ba96d8da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_indices = np.load('val-indices.npy')\n",
    "val_indices.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c560d264b4a6907",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_generator(lambda: get_data(val_indices), output_signature=ds_sig)\n",
    "val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(val_indices.shape[0] * (n_times - TIME_STEPS)))\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "len(val_ds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3bf34c9e6900cfd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a341ddc546b37dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_ds)\n",
    "val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "val_y = np.concatenate([y for _, y in val_ds], axis=0)\n",
    "val_y.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb99dd37be0ed1a"
  },
  {
   "cell_type": "markdown",
   "id": "791915d7fc0cdd28",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "Run the first 2 cells from the evaluation section to load the model first. Some cells here also require other cells from the training section to be ran, to define to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb1044-519e-4fa2-b740-4f714917b09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFER_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928518c35eff450",
   "metadata": {},
   "source": [
    "Run the model to predict the landscape of a random patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ac61b70a73ed",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 10192 # random.randint(2000, 16000)\n",
    "y = 14735 # random.randint(3000, 32000)\n",
    "patch = images[-TIME_STEPS:, x:x + PATCH_SIZE, y:y + PATCH_SIZE]\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 50))\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(patch[i, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c409d86feab4c",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch = one_hot(patch)\n",
    "patch = np.expand_dims(patch, axis=0)\n",
    "result = model.predict(patch)\n",
    "result = np.squeeze(result, axis=0)\n",
    "result = np.argmax(result, axis=-1)\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827515618af7e20",
   "metadata": {},
   "source": [
    "Set parameters for generating the large prediction map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8565a88c8becd4",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = 10192 # random.randint(2000, 16000)\n",
    "y = 14735 # random.randint(3000, 32000)\n",
    "size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea515835d46cf89",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_generator(step):\n",
    "    for i in range(0, PATCH_SIZE * size - (PATCH_SIZE - step), step):\n",
    "        for j in range(0, PATCH_SIZE * size - (PATCH_SIZE - step), step):\n",
    "            patch = images[-TIME_STEPS:, x + i:x + i + PATCH_SIZE, y + j:y + j + PATCH_SIZE]\n",
    "            patch = one_hot(patch)\n",
    "            yield i, j, patch\n",
    "\n",
    "def batch_patch_generator(gen):\n",
    "    coords = []\n",
    "    batch = []\n",
    "    n = 0\n",
    "    for i, j, patch in gen:\n",
    "        coords.append((i, j))\n",
    "        batch.append(patch)\n",
    "        n += 1\n",
    "        if n == INFER_BATCH_SIZE:\n",
    "            yield np.array(coords), np.stack(batch)\n",
    "            coords = []\n",
    "            batch = []\n",
    "            n = 0\n",
    "    if n:\n",
    "        yield np.array(coords), np.stack(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bed4045a4d2d62",
   "metadata": {},
   "source": [
    "Construct a large map of predictions by running the model on several consecutive patches without overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0075934694d1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = np.zeros((PATCH_SIZE * size, PATCH_SIZE * size), dtype=np.uint8)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(range(0, PATCH_SIZE * size, PATCH_SIZE)) ** 2 / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(patch_generator(PATCH_SIZE))\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    result = np.argmax(result, axis=-1)\n",
    "    for i in range(len(coords)):\n",
    "        image[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] = result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f75633600cb5c",
   "metadata": {},
   "source": [
    "Construct a large map of predictions with overlap. The probability distributions at each pixel are summed with contribution from all overlapping patches before `argmax` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05bc37c28256c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = np.zeros((PATCH_SIZE * size, PATCH_SIZE * size, n_categories), dtype=np.float32)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(range(0, PATCH_SIZE * size - OVERLAP_SIZE, OVERLAP_SIZE)) ** 2 / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(patch_generator(OVERLAP_SIZE))\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    for i in range(len(coords)):\n",
    "        probs[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] += result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "image = np.argmax(probs, axis=-1)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7b5c64da46e87",
   "metadata": {},
   "source": [
    "Build a map over the entire area of interest. Be sure to run the cell creating `zero_prefix` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a9242d3f95636",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(0, images.shape[1] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "    for j in range(0, images.shape[2] - PATCH_SIZE, OVERLAP_SIZE):\n",
    "        zeros = get_zero_pixels(i, j)\n",
    "        if zeros == PATCH_SIZE * PATCH_SIZE:\n",
    "            continue  # Don't need to run any prediction if the slice is entirely blank\n",
    "        indices.append((i, j))\n",
    "del zero_prefix\n",
    "indices = np.array(indices)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929acea6a9e89e2e",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_patch_generator():\n",
    "    for i, j in indices:\n",
    "        patch = images[-TIME_STEPS:, i:i + PATCH_SIZE, j:j + PATCH_SIZE]\n",
    "        patch = one_hot(patch)\n",
    "        yield i, j, patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e169bae2ccc08",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = np.zeros(images.shape[1:] + (n_categories,), dtype=np.float32)\n",
    "progbar = keras.utils.Progbar(np.ceil(len(indices) / INFER_BATCH_SIZE), unit_name='batch')\n",
    "gen = batch_patch_generator(full_patch_generator())\n",
    "n = 0\n",
    "for coords, batch in gen:\n",
    "    result = model.predict(batch, verbose=0)\n",
    "    for i in range(len(coords)):\n",
    "        probs[coords[i, 0]:coords[i, 0] + PATCH_SIZE, coords[i, 1]:coords[i, 1] + PATCH_SIZE] += result[i]\n",
    "    n += 1\n",
    "    progbar.update(n)\n",
    "image = np.argmax(probs, axis=-1)\n",
    "del probs\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3024e-8ffe-444a-b60e-3e29c2cb238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.vectorize(lambda x: categories[x])(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2a8baef395ffd",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('full-map.npy', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b4117-03bd-43c3-aa94-11154f0d6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(image, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc274e582e0181",
   "metadata": {},
   "source": [
    "Load the numpy array and save the data as a GeoTIFF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143be246c12991f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = np.load('full-map.npy')\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80980b-c62b-46e6-9e6c-64d14a4a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('data/CONUS2001_ClipAOI.tif') as src:\n",
    "    profile = src.profile.copy()\n",
    "    with rasterio.open('prediction-1.tif', 'w', **profile) as dst:\n",
    "        dst.write(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa97d813e3bf357",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m117"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
